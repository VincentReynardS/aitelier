{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6bcedef",
   "metadata": {},
   "source": [
    "# Interaction Data Generation\n",
    "Using the transactions data is not enough. This is because the transactions data only provide information about which items the customers were buying. However, to train a recommender system, we need training samples that indicates that the customer interacts with the item (positive signal) and also samples that indicates otherwise (negative signal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb7780",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12799fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincent_reynard/Developments/PROJECTS/aitelier/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path('../data/')\n",
    "processed_data_path = data_path / 'processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede93296",
   "metadata": {},
   "source": [
    "### Data Sampling\n",
    "The transactions dataset contains ~30 million rows. It will take a very long time to train our model with this many data. For demonstration purpose, let's start with transactions from 1000 customers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1000 customers from 1356119 total customers.\n",
      "Number of transactions for all customers: 31788324\n",
      "Number of transactions for the 1000 sampled customers: 24341\n"
     ]
    }
   ],
   "source": [
    "CUSTOMER_DATASET_SIZE = 1000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "transactions_df = pd.read_pickle(processed_data_path / 'transactions.pkl')\n",
    "customers_df = pd.read_pickle(processed_data_path / 'customers.pkl')\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\n",
    "    f'Sampling {CUSTOMER_DATASET_SIZE} customers from {len(customers_df)} total customers.'\n",
    ")\n",
    "customers_df = customers_df.sample(n=CUSTOMER_DATASET_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f'Number of transactions for all customers: {len(transactions_df)}')\n",
    "transactions_df = pd.merge(\n",
    "    transactions_df, customers_df[['customer_id']], how='inner', on='customer_id'\n",
    ")\n",
    "print(\n",
    "    f'Number of transactions for the {CUSTOMER_DATASET_SIZE} sampled customers: {len(transactions_df)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4b8c3",
   "metadata": {},
   "source": [
    "Before we proceed, let's save the sampled `customers_df` and `transactions_df` as pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193344a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.to_pickle(data_path / 'processed' / 'sampled_customers.pkl')\n",
    "transactions_df.to_pickle(data_path / 'processed' / 'sampled_transactions.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3c763",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd02942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sampled customers: 1000\n",
      "Number of transactions from the sampled customers: 24341\n"
     ]
    }
   ],
   "source": [
    "# Load sampled data\n",
    "sampled_transactions_df: pd.DataFrame = pd.read_pickle(\n",
    "    data_path / 'processed' / 'sampled_transactions.pkl'\n",
    ")\n",
    "sampled_customers_df: pd.DataFrame = pd.read_pickle(\n",
    "    data_path / 'processed' / 'sampled_customers.pkl'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Number of unique sampled customers: {len(sampled_customers_df['customer_id'].unique())}\"\n",
    ")\n",
    "print(\n",
    "    f'Number of transactions from the sampled customers: {len(sampled_transactions_df)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060f4c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14013"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles = sampled_transactions_df['article_id'].unique().tolist()\n",
    "len(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08198519",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_IGNORES = 40\n",
    "MAX_IGNORES = 60\n",
    "CLICK_BEFORE_PURCHASE_PROB = 0.9\n",
    "EXTRA_CLICKS_PROB = 0.95\n",
    "MIN_EXTRA_CLICKS = 5\n",
    "MAX_EXTRA_CLICKS = 15\n",
    "\n",
    "\n",
    "def generate_timestamps(base_timestamp, count, min_hours, max_hours) -> np.ndarray:\n",
    "    hours = np.random.randint(min_hours, max_hours, size=count)\n",
    "    return base_timestamp - (hours * 3600000)\n",
    "\n",
    "\n",
    "for customer_id in sampled_customers_df['customer_id'].tolist():\n",
    "    customer_purchases = sampled_transactions_df[\n",
    "        sampled_transactions_df['customer_id'] == customer_id\n",
    "    ]\n",
    "\n",
    "    if len(customer_purchases) == 0:\n",
    "        continue\n",
    "\n",
    "    customer_articles = {'purchased': set(), 'clicked': set(), 'ignored': set()}\n",
    "    last_purchase_timestamp = customer_purchases['t_dat'].max()\n",
    "    interactions = []\n",
    "\n",
    "    # First, we generate multiple ignores\n",
    "    num_ignores = np.random.randint(MIN_IGNORES, MAX_IGNORES)\n",
    "    if all_articles and num_ignores > 0:\n",
    "        ignore_timestamps = generate_timestamps(\n",
    "            base_timestamp=last_purchase_timestamp,\n",
    "            count=num_ignores,\n",
    "            min_hours=1,\n",
    "            max_hours=96,\n",
    "        ).tolist()\n",
    "        selected_ignores = np.random.choice(\n",
    "            all_articles, size=min(num_ignores, len(all_articles)), replace=False\n",
    "        ).tolist()\n",
    "\n",
    "        # Generate multiple sets of ignores to increase the count\n",
    "        for timestamp, article_id in zip(ignore_timestamps, selected_ignores):\n",
    "            # Add 1-2 ignore events for the same article\n",
    "            num_ignore_events = np.random.randint(1, 3)\n",
    "            for _ in range(num_ignore_events):\n",
    "                # Add some hours difference\n",
    "                ignore_timestamp = timestamp - np.random.randint(1, 12) * 3600000\n",
    "                interactions.append(\n",
    "                    {\n",
    "                        't_dat': ignore_timestamp,\n",
    "                        'customer_id': customer_id,\n",
    "                        'article_id': article_id,\n",
    "                        'interaction_score': 0,\n",
    "                        'prev_article_id': None,\n",
    "                    }\n",
    "                )\n",
    "            customer_articles['ignored'].add(article_id)\n",
    "\n",
    "    # Second, we process the purchases and their clicks\n",
    "    for row in customer_purchases.iterrows():\n",
    "        # See https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html\n",
    "        # on why we need [1] to obtain the data.\n",
    "        purchase_timestamp = row[1]['t_dat']\n",
    "        article_id = row[1]['article_id']\n",
    "\n",
    "        # Add clicks before purchase\n",
    "        if np.random.random() < CLICK_BEFORE_PURCHASE_PROB:\n",
    "            num_pre_clicks = np.random.randint(1, 3)\n",
    "            for _ in range(num_pre_clicks):\n",
    "                click_timestamp = generate_timestamps(\n",
    "                    base_timestamp=purchase_timestamp,\n",
    "                    count=1,\n",
    "                    min_hours=1,\n",
    "                    max_hours=48,\n",
    "                )[0]\n",
    "                interactions.append(\n",
    "                    {\n",
    "                        't_dat': click_timestamp,\n",
    "                        'customer_id': customer_id,\n",
    "                        'article_id': article_id,\n",
    "                        'interaction_score': 1,\n",
    "                        'prev_article_id': None,\n",
    "                    }\n",
    "                )\n",
    "                customer_articles['clicked'].add(article_id)\n",
    "\n",
    "        # Add purchase\n",
    "        interactions.append(\n",
    "            {\n",
    "                't_dat': purchase_timestamp,\n",
    "                'customer_id': customer_id,\n",
    "                'article_id': article_id,\n",
    "                'interaction_score': 2,\n",
    "                'prev_article_id': None,\n",
    "            }\n",
    "        )\n",
    "        customer_articles['purchased'].add(article_id)\n",
    "\n",
    "    # Third, we generate extra clicks\n",
    "    if np.random.random() < EXTRA_CLICKS_PROB:\n",
    "        num_extra_clicks = np.random.randint(MIN_EXTRA_CLICKS, MAX_EXTRA_CLICKS)\n",
    "        available_for_clicks = list(\n",
    "            set(all_articles)\n",
    "            - customer_articles['purchased']\n",
    "            - customer_articles['clicked']\n",
    "            - customer_articles['ignored']\n",
    "        )\n",
    "\n",
    "        if available_for_clicks and num_extra_clicks > 0:\n",
    "            for _ in range(num_extra_clicks):\n",
    "                click_timestamps = generate_timestamps(\n",
    "                    base_timestamp=last_purchase_timestamp,\n",
    "                    count=num_extra_clicks,\n",
    "                    min_hours=1,\n",
    "                    max_hours=72,\n",
    "                )\n",
    "                selected_clicks = np.random.choice(\n",
    "                    available_for_clicks,\n",
    "                    size=min(num_extra_clicks, len(available_for_clicks)),\n",
    "                    replace=False,\n",
    "                ).tolist()\n",
    "\n",
    "                for timestamp, article_id in zip(click_timestamps, selected_clicks):\n",
    "                    interactions.append(\n",
    "                        {\n",
    "                            't_dat': timestamp,\n",
    "                            'customer_id': customer_id,\n",
    "                            'article_id': article_id,\n",
    "                            'interaction_score': 1,\n",
    "                            'prev_article_id': None,\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df80dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.DataFrame(interactions)\n",
    "sorted_df = interactions_df.sort_values(['customer_id', 't_dat'])\n",
    "\n",
    "final_df = sorted_df.assign(\n",
    "    prev_article_id=sorted_df.groupby('customer_id')['article_id']\n",
    "    .shift(1)\n",
    "    .fillna('START')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e249512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>interaction_score</th>\n",
       "      <th>prev_article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1579885200000</td>\n",
       "      <td>6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...</td>\n",
       "      <td>455014028</td>\n",
       "      <td>0</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1579888800000</td>\n",
       "      <td>6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...</td>\n",
       "      <td>834002002</td>\n",
       "      <td>0</td>\n",
       "      <td>455014028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1579888800000</td>\n",
       "      <td>6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...</td>\n",
       "      <td>788115001</td>\n",
       "      <td>0</td>\n",
       "      <td>834002002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1579896000000</td>\n",
       "      <td>6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...</td>\n",
       "      <td>579302009</td>\n",
       "      <td>0</td>\n",
       "      <td>788115001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1579906800000</td>\n",
       "      <td>6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...</td>\n",
       "      <td>834002002</td>\n",
       "      <td>0</td>\n",
       "      <td>579302009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            t_dat                                        customer_id  \\\n",
       "31  1579885200000  6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...   \n",
       "23  1579888800000  6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...   \n",
       "39  1579888800000  6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...   \n",
       "68  1579896000000  6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...   \n",
       "24  1579906800000  6d6e91ea618beec6fa2843d5fbf25a9164c6318abdd715...   \n",
       "\n",
       "   article_id  interaction_score prev_article_id  \n",
       "31  455014028                  0           START  \n",
       "23  834002002                  0       455014028  \n",
       "39  788115001                  0       834002002  \n",
       "68  579302009                  0       788115001  \n",
       "24  834002002                  0       579302009  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf6043",
   "metadata": {},
   "source": [
    "### Save the Engineered Features\n",
    "Lastly, we pickle it to save the generated interactions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1ce2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle(data_path / 'processed' / 'interactions.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aitelier (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
